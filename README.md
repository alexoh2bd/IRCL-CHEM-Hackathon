# ChemLit-QA Instruction-Following Retrieval Dataset

## Overview

This project creates an instruction-following retrieval dataset for chemistry domain using the ChemLit-QA dataset as a foundation. Inspired by the **Promptriever** methodology, we generate instruction-positive and instruction-negative passage pairs to train retrieval models that can follow nuanced relevance criteria beyond simple query matching.


## [Dataset Link](https://drive.google.com/drive/folders/1StPzvw1RTF0DwVVkq9_rDkejmFQHyQ7x?usp=sharing)

## Motivation & Reasoning

### The Problem
Traditional retrieval systems operate on basic query-document similarity, but real-world information needs often require more nuanced relevance criteria. For example, a chemistry researcher might need passages that not only discuss "polymer crystallization" but specifically include "temperature ranges and kinetic parameters."

### Our Approach
Following the Promptriever framework, we:
1. **Generate Instructions**: Create natural language instructions that narrow relevance definitions
2. **Generate Passage Candidates**: Produce both instruction-positive and instruction-negative examples
3. **Create Training Data**: Build high-quality instruction-following retrieval datasets

## Dataset Information

**Base Dataset**: [ChemLit-QA](https://github.com/geemi725/ChemLit-QA)
- 1,000+ expert-validated chemistry Q&A pairs
- Covers diverse chemistry topics with rigorous expert evaluation
- Provides rich context for generating nuanced instructions

**Generated Dataset Structure**:
```
ChemLit-QA ID:
Query
Original Positive
Instruction
Response
 - Response Id
 - Label: Positive or Negative
 - Response Passage: (Generated based on the Query + Instruction Pair)
 - Confidence Score (Generated by the Model)
```

## Methodology

### Step 1: Instruction Generation
Using `text.yaml` template with Cerebras Qwen-3-32B model:
- **Input**: Chemistry question + answer pair
- **Output**: Natural language instruction that adds qualifications
- **Styles**: Persona, negation, background, generic
- **Lengths**: Short, medium, long, very long


### Step 2: Passage Generation
Using `responses.yaml` template:
- Generate 1 instruction-positive passage
- Generate 5-6 instruction-negative passages
- Each negative follows specific failure modes:
  - Omission of required details
  - Different interpretation
  - Mentions excluded elements
  - Partial answers
  - Outdated information
  - Unsupported inference

### Step 3: Quality Filtering
I intended to use FollowIR-7B model for validation:
- **Instruction Positives**: Must be relevant to both query AND instruction
- **Instruction Negatives**: Must be relevant to query but NOT to instruction
- Confidence scoring for each relevance decision
- Memory-efficient batch processing with checkpointing 

## Observations

The **objective** of our instructions is important. 
Similar to the paper, I appended additional instructions to the original query which altered the prompt’s relevance query. For a domain-specific QA dataset, the resulting prompt could hold a variety of changes to the original query’s relevance criteria. How do we want to change the query’s relevance criteria? Do we:
- Add specified criteria and required details?
- Provide contextual precision for the prompt?
- Add complexity through additional information and ask for chain-of-thought reasoning steps? <br>

How do we construct the instruction-negative responses? Where should instruction-negative responses fall short of complete answers? For this scenario, distinguish not just “correct vs. random,” but “correct vs. superficially plausible but wrong.” The negatives should require the model to know chemistry to tell them apart from positives.
To answer these questions we need to properly frame our objective. By creating a contrastive learning dataset, what exactly are we training the model to do?  In the scenario of the Chemistry Literature data example, do we want the model to understand chemistry concepts with higher precision, or do we want the model to improve its reasoning surrounding chemistry concepts? 

## Example Walkthrough
### Objective
The goal of this dataset is to train models to develop a deeper, instruction-aware understanding of niche chemistry concepts and their experimental dynamics.
### Query
How does the fluorescence quenching percentage of JUC-557-nanosheet at pH 1 compare to its quenching percentage at pH 14?
### Instruction
A relevant passage must explicitly report and detail the fluorescence quenching percentages of JUC-557-nanosheet under acidic (pH 1) and basic (pH 14) conditions, including the measurement method (e.g., steady-state or time-resolved fluorimetry), the concentration of nanosheet solutions analyzed, and the specific buffer or solvent systems used. It should also clarify whether the comparison accounts for potential interferences from pH-dependent structural changes or surface functionalization effects. Responses that only generalize quenching trends without this level of methodological and contextual precision at pH 1 and 14 are not relevant.
### Prompt Quality Rationale:
- Tightens the definition of “relevant” from basic comparison → quantitative + methodological + contextual precision.
- Provides multiple avenues for constructing meaningful negatives (e.g., omit % values, omit methods, omit buffers, only mention trends).
- Forces the model to learn source-material-level reasoning, not superficial answers.
- Note: The instruction is very dense. If you scale up, you might want to simplify phrasing or modularize requirements (e.g., numbered bullet points), otherwise generation may drift or miss pieces.
### Instruction Negative Examples:
- **Omission**: Mentions trend (acid > base) but no % values, no method, no buffers.
- **Partial Answer**: Reports only one pH condition (e.g., quenching at pH 1) but ignores the comparison to pH 14.
- **Different Interpretation**: Says quenching is due to aggregation or solvent polarity instead of protonation/electron transfer (plausible but doesn’t match instruction).
- **Mentions Excluded Element**: Talks about JUC-557 at neutral pH or with a different metal-organic framework, not the specified 1 vs. 14.
- **Outdated / Unsupported**: References “an early report suggested ~50% quenching” without specifying method, concentration, or buffers.
- **Over-generalization**: “All nanosheets quench more in acid than base” — plausible, but lacks specificity to the JUC-557 case.


This question is vital in how we prompt the LM and assess the resulting dataset. 



## Additional Example

```yaml
Original Query: What happens to the separation of compounds when the pressure is increased?
Original Response: Some compounds separate better at higher pressure
Generated Instruction: A relevant passage must describe how increased pressure chemically or physically influences <br>
the separation of compounds, specifically addressing the type of compounds involved (e.g., volatile, polar, or gas-phase species) <br>
 and the separation mechanism (e.g., distillation, gas solubility, or chromatography). General statements about pressure effects <br>
without tying them to molecular interactions, phase behavior, or method-specific adjustments are not sufficient for relevance.<br><br>
I-Positive Response: Increasing pressure affects compound separation differently depending on the method. In distillation, higher<br>
pressure raises boiling points for volatile compounds, making their separation less efficient due to reduced vapor-liquid equilibrium <br>
differences. For gas-phase compounds in gas chromatography, elevated column inlet pressure accelerates flow rates, shortening retention <br>
times and decreasing separation efficiency, while reducing outlet pressure improves resolution for non-polar compounds by prolonging <br>
interaction with the stationary phase. Polar compounds in liquid chromatography exhibit altered solubility at higher pressures, impacting <br>
partitioning between phases. Additionally, gas solubility in liquids increases with pressure (Henry’s Law), which complicates methods <br>
like pressure-sensitive extraction. Source chunks ID1-3 support these specific compound-type and mechanism-dependent effects.
<br> <br>

I-Negative Response 1: Pressure influences chemical equilibria by favoring low-molar-volume reactions. In high-pressure environments, compounds often resist phase changes. This can stabilize reaction intermediates but may also hinder equilibration.
I-Negative Response 2: Elevated pressure reduces the volatility of compounds, narrowing boiling point differences. In vacuum distillations, lower pressure increases separation efficiency for volatile mixtures by lowering their boiling points.
```


Configuration
```python
# Ingestion Settings
BATCH_SIZE = 20              # Rows to write at once
CHECKPOINT_INTERVAL = 5      # Save state every N rows
MAX_ROWS = None             # Limit processing (None = all)

# Model Settings
MODEL = "qwen-3-32b"        # Instruction generation model
FILTER_MODEL = "jhu-clsp/FollowIR-7B"  # Relevance filtering model
```

## Results & Outcomes

### Dataset Statistics
- **Source**: 1,000+ ChemLit-QA entries
- **Generated**: ~6,000+ instruction-passage pairs

- **Coverage**: Diverse chemistry domains with expert-validated foundations

### Quality Metrics
- **Instruction Diversity**: 4 styles × 4 lengths = 16 instruction variants
- **Failure Mode Coverage**: 6 distinct negative passage types

##### INSTRUCTION POSITIVE Prompt-Response Pairs 
- **Count**: 867 (17.2% of total)
- **Mean**: 0.9553
- **Median**: 0.9500
- **Std Dev**: 0.0259
- **Variance**: 0.0007
- **Range**: 0.8000 - 1.0000
- **IQR**: 0.9500 - 0.9800
- **Skewness**: -1.0970
- **Kurtosis**: 3.5059

##### INSTRUCTION NEGATIIVE Prompt-Response Pairs
- **Count**: 4174 (82.7% of total)
- **Mean**: 0.5290
- **Median**: 0.5700
- **Std Dev**: 0.1829
- **Variance**: 0.0334
- **Range**: 0.0000 - 1.0000
- **IQR**: 0.4000 - 0.6800
- **Skewness**: -0.3999
- **Kurtosis**: -0.7277


### Key Innovations
1. **Chemistry-Specific Instructions**: Domain-aware relevance criteria
2. **Automated Quality Control**: Model-based filtering pipeline
3. **Scalable Architecture**: Memory-efficient processing for large datasets
4. **Comprehensive Validation**: Multi-stage quality assurance

### Pitfalls and Improvements
1. **Inconsistent LM Responses and Data Loss**: Inconsistent response formatting led to loss of responses and pipeline issues. $903 / 1025$ entries logged. 
2. **Incomplete Implementation of the Promptriever Data Generation Workflow**: Intended to use the instruction retrieval encoder FollowIR to assess the relevance of prompt response pairs and filter and filter positive and negative responses out of range. 


## Usage

### Generate Dataset
```bash
# Run instruction-following dataset generation
python src/data/ingestion_v2.py

# Filter generated passages
python src/data/filterv2.py
```

## Future Work

1. **Evaluation Framework**: Implement retrieval performance benchmarks
2. **Model Training**: Fine-tune retrieval models on generated dataset
3. **Domain Expansion**: Extend methodology to other scientific domains
4. **Interactive Filtering**: Human-in-the-loop quality validation

## Citation

This work builds upon:
- **ChemLit-QA**: Expert-validated chemistry Q&A dataset
- **Promptriever**: Instruction-following retrieval methodology
- **FollowIR**: Instruction-aware relevance modeling

--- 
